import os
import uuid
from datetime import datetime, timedelta
import random
from supabase import create_client, Client
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

url: str = os.environ.get("SUPABASE_URL")
key: str = os.environ.get("SUPABASE_KEY")

if not url or not key:
    raise ValueError("‚ö†Ô∏è ERREUR: Les cl√©s SUPABASE_URL ou SUPABASE_KEY sont manquantes.")

supabase: Client = create_client(url, key)

print("üå± D√©marrage du remplissage de la base de donn√©es (Historique R√©aliste 5 mois)...")

# --- DONN√âES HISTORIQUES R√âALISTES (Derniers 5 mois) ---
# Bas√© sur les tendances r√©elles : Zero-ETL, IA G√©n√©rative, Gouvernance, Vector Search.
historical_news = [
    # ==============================
    # AWS (Amazon Web Services)
    # ==============================
    {
        "title": "AWS annonce Amazon Q pour l'int√©gration de donn√©es (Zero-ETL)",
        "summary": "Amazon Q permet d√©sormais de g√©n√©rer automatiquement des pipelines Zero-ETL entre Aurora et Redshift en langage naturel, simplifiant l'int√©gration de donn√©es sans code.",
        "provider": "AWS",
        "service": "AWS Glue / Redshift",
        "category": "ETL",
        "impact_level": 3,
        "impact_analysis": "R√©volutionne la productivit√© des Data Engineers en automatisant la cr√©ation de pipelines complexes.",
        "link": "https://aws.amazon.com/blogs/big-data/amazon-q-zero-etl-integration",
        "days_ago": 15
    },
    {
        "title": "Amazon Redshift ajoute le support du Vector Search pour le RAG",
        "summary": "Redshift supporte maintenant nativement la recherche vectorielle, permettant d'interroger des bases de connaissances pour les applications d'IA g√©n√©rative directement en SQL.",
        "provider": "AWS",
        "service": "Redshift",
        "category": "ML",
        "impact_level": 3,
        "impact_analysis": "Permet de construire des applications RAG performantes directement sur le Data Warehouse sans base vectorielle d√©di√©e.",
        "link": "https://aws.amazon.com/about-aws/whats-new/2024/12/redshift-vector-search-preview",
        "days_ago": 45
    },
    {
        "title": "AWS Glue Data Quality ajoute la d√©tection d'anomalies par IA",
        "summary": "Nouvelle fonctionnalit√© utilisant le ML pour d√©tecter automatiquement les d√©rives de qualit√© (data drift) dans les Data Lakes S3 sans r√®gles manuelles.",
        "provider": "AWS",
        "service": "AWS Glue",
        "category": "Gouvernance",
        "impact_level": 2,
        "impact_analysis": "Am√©liore la fiabilit√© des donn√©es pour les mod√®les ML sensibles.",
        "link": "https://aws.amazon.com/blogs/big-data/glue-data-quality-anomaly-detection",
        "days_ago": 60
    },
    {
        "title": "Amazon S3 Express One Zone : Latence ultra-faible pour Spark",
        "summary": "Lancement d'une nouvelle classe de stockage S3 offrant des performances 10x sup√©rieures pour les charges de travail analytiques intensives comme Spark et EMR.",
        "provider": "AWS",
        "service": "S3",
        "category": "Stockage",
        "impact_level": 3,
        "impact_analysis": "Game changer pour les co√ªts et la performance des traitements Big Data temps r√©el.",
        "link": "https://aws.amazon.com/s3/express-one-zone/",
        "days_ago": 110
    },
    {
        "title": "EMR Serverless supporte d√©sormais les images Docker personnalis√©es",
        "summary": "Les d√©veloppeurs peuvent maintenant utiliser leurs propres images Docker pour ex√©cuter des jobs Spark et Hive sur EMR Serverless, offrant plus de flexibilit√©.",
        "provider": "AWS",
        "service": "EMR",
        "category": "Compute",
        "impact_level": 2,
        "impact_analysis": "Facilite la migration des workloads legacy vers le Serverless.",
        "link": "https://aws.amazon.com/about-aws/whats-new/2024/10/emr-serverless-custom-images",
        "days_ago": 90
    },
    {
        "title": "Amazon Athena supporte d√©sormais les UDFs en Java et Python",
        "summary": "Athena permet l'ex√©cution de fonctions d√©finies par l'utilisateur (UDF) complexes directement dans les requ√™tes SQL serverless.",
        "provider": "AWS",
        "service": "Athena",
        "category": "Compute",
        "impact_level": 1,
        "impact_analysis": "√âtend les capacit√©s SQL standard pour des transformations complexes √† la vol√©e.",
        "link": "https://aws.amazon.com/blogs/big-data/athena-udf-support",
        "days_ago": 130
    },
    {
        "title": "AWS DataZone : Gouvernance automatis√©e pour les Data Mesh",
        "summary": "DataZone simplifie le partage de donn√©es entre comptes AWS avec une gestion fine des acc√®s et un catalogue m√©tier unifi√©.",
        "provider": "AWS",
        "service": "DataZone",
        "category": "Gouvernance",
        "impact_level": 2,
        "impact_analysis": "Simplifie massivement l'impl√©mentation d'une architecture Data Mesh.",
        "link": "https://aws.amazon.com/datazone/",
        "days_ago": 20
    },
     {
        "title": "Amazon OpenSearch Serverless supporte les collections de vecteurs",
        "summary": "Mise √† l'√©chelle automatique pour les collections vectorielles, facilitant le d√©ploiement d'applications de recherche s√©mantique.",
        "provider": "AWS",
        "service": "OpenSearch",
        "category": "ML",
        "impact_level": 2,
        "impact_analysis": "R√©duit la complexit√© op√©rationnelle des moteurs de recherche vectoriels.",
        "link": "https://aws.amazon.com/opensearch-service/serverless-vector-engine/",
        "days_ago": 50
    },

    # ==============================
    # AZURE (Microsoft)
    # ==============================
    {
        "title": "Microsoft Fabric : Disponibilit√© G√©n√©rale (GA) annonc√©e",
        "summary": "La plateforme unifi√©e de donn√©es Microsoft Fabric est d√©sormais en disponibilit√© g√©n√©rale, int√©grant Data Factory, Synapse et Power BI en une seule interface SaaS.",
        "provider": "Azure",
        "service": "Microsoft Fabric",
        "category": "Gouvernance",
        "impact_level": 3,
        "impact_analysis": "L'annonce la plus importante de l'ann√©e pour l'√©cosyst√®me Microsoft Data.",
        "link": "https://blog.fabric.microsoft.com/en-us/blog/fabric-ga-announcement",
        "days_ago": 30
    },
    {
        "title": "Azure AI Search : Augmentation massive des limites vectorielles",
        "summary": "Azure AI Search supporte d√©sormais des milliards de vecteurs par index avec une latence milliseconde, optimis√© pour les applications RAG √† grande √©chelle.",
        "provider": "Azure",
        "service": "AI Search",
        "category": "ML",
        "impact_level": 2,
        "impact_analysis": "Essentiel pour les entreprises d√©ployant des Copilots sur de grandes bases de connaissances.",
        "link": "https://azure.microsoft.com/updates/ai-search-vector-capacity",
        "days_ago": 40
    },
    {
        "title": "OneLake Shortcuts : Support pour S3 et Google Cloud Storage",
        "summary": "Microsoft Fabric OneLake permet de virtualiser des donn√©es stock√©es sur AWS S3 et GCP sans les d√©placer, renfor√ßant la strat√©gie multi-cloud.",
        "provider": "Azure",
        "service": "Microsoft Fabric",
        "category": "Stockage",
        "impact_level": 3,
        "impact_analysis": "√âlimine les co√ªts d'egress et les pipelines de copie de donn√©es complexes.",
        "link": "https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts",
        "days_ago": 65
    },
    {
        "title": "Azure Synapse : Copilot pour l'√©criture SQL et PySpark",
        "summary": "L'assistant IA Copilot est int√©gr√© dans Synapse Studio pour aider √† √©crire, d√©boguer et optimiser le code SQL et Python.",
        "provider": "Azure",
        "service": "Synapse Analytics",
        "category": "ML",
        "impact_level": 2,
        "impact_analysis": "Boost de productivit√© significatif pour les d√©veloppeurs Data.",
        "link": "https://azure.microsoft.com/updates/synapse-copilot-preview",
        "days_ago": 100
    },
    {
        "title": "Azure Databricks : Support de DBR 14.3 LTS et Spark 3.5",
        "summary": "Nouvelle version Long Term Support de Databricks Runtime incluant les derni√®res optimisations de Spark et Photon.",
        "provider": "Azure",
        "service": "Databricks",
        "category": "Compute",
        "impact_level": 1,
        "impact_analysis": "Mise √† jour standard recommand√©e pour la stabilit√© et la performance.",
        "link": "https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/14.3lts",
        "days_ago": 140
    },
    {
        "title": "Cosmos DB for MongoDB vCore : Recherche vectorielle int√©gr√©e",
        "summary": "Cosmos DB ajoute le support natif des vecteurs pour l'API MongoDB, permettant de construire des apps IA sans changer de base de donn√©es.",
        "provider": "Azure",
        "service": "Cosmos DB",
        "category": "Stockage",
        "impact_level": 2,
        "impact_analysis": "Convergence des bases de donn√©es transactionnelles et vectorielles.",
        "link": "https://azure.microsoft.com/updates/cosmos-db-mongodb-vector",
        "days_ago": 85
    },
    {
        "title": "Azure Stream Analytics : Support de Delta Lake en sortie",
        "summary": "√âcriture directe des flux de donn√©es temps r√©el au format Delta Lake, optimisant l'analyse post-traitement dans Synapse et Databricks.",
        "provider": "Azure",
        "service": "Stream Analytics",
        "category": "ETL",
        "impact_level": 2,
        "impact_analysis": "Simplifie l'architecture Lambda/Kappa sur Azure.",
        "link": "https://azure.microsoft.com/updates/stream-analytics-delta",
        "days_ago": 120
    },

    # ==============================
    # GCP (Google Cloud Platform)
    # ==============================
    {
        "title": "BigQuery Omni : Cross-Cloud Joins disponibles",
        "summary": "BigQuery permet d√©sormais de faire des jointures SQL entre des donn√©es locales et des donn√©es stock√©es sur AWS S3 ou Azure Blob sans d√©placement massif.",
        "provider": "GCP",
        "service": "BigQuery",
        "category": "Compute",
        "impact_level": 3,
        "impact_analysis": "Fonctionnalit√© cl√© pour les strat√©gies v√©ritablement multi-cloud.",
        "link": "https://cloud.google.com/bigquery/docs/omni-introduction",
        "days_ago": 25
    },
    {
        "title": "Google Cloud Spanner Data Boost : Analytique sans impact sur la prod",
        "summary": "Data Boost permet d'ex√©cuter des requ√™tes analytiques lourdes sur Spanner en utilisant des ressources de calcul ind√©pendantes, sans ralentir les transactions.",
        "provider": "GCP",
        "service": "Spanner",
        "category": "Compute",
        "impact_level": 2,
        "impact_analysis": "Permet le HTAP (Hybrid Transactional/Analytical Processing) √† grande √©chelle.",
        "link": "https://cloud.google.com/spanner/docs/databoost/databoost-overview",
        "days_ago": 55
    },
    {
        "title": "Gemini in Looker : G√©n√©ration de tableaux de bord par chat",
        "summary": "Looker int√®gre Gemini pour permettre aux utilisateurs m√©tier de cr√©er des visualisations et des rapports complets simplement en conversant avec l'IA.",
        "provider": "GCP",
        "service": "Looker",
        "category": "ML",
        "impact_level": 2,
        "impact_analysis": "D√©mocratisation de la BI pour les utilisateurs non techniques.",
        "link": "https://cloud.google.com/blog/products/business-intelligence/gemini-in-looker",
        "days_ago": 10
    },
    {
        "title": "BigQuery Studio : Environnement unifi√© pour SQL et Python",
        "summary": "Lancement de BigQuery Studio, un IDE unique pour l'analyse SQL, le Machine Learning et la programmation Python (Notebooks Colab int√©gr√©s).",
        "provider": "GCP",
        "service": "BigQuery",
        "category": "Gouvernance",
        "impact_level": 2,
        "impact_analysis": "Unifie l'exp√©rience d√©veloppeur Data et Data Scientist.",
        "link": "https://cloud.google.com/blog/products/data-analytics/bigquery-studio-generative-ai",
        "days_ago": 95
    },
    {
        "title": "AlloyDB AI : Vecteurs et ML int√©gr√©s pour PostgreSQL",
        "summary": "AlloyDB pour PostgreSQL int√®gre des capacit√©s vectorielles natives pour construire des applications d'IA g√©n√©rative ultra-rapides.",
        "provider": "GCP",
        "service": "AlloyDB",
        "category": "Stockage",
        "impact_level": 2,
        "impact_analysis": "Concurrent direct de pgvector avec les performances de l'infrastructure Google.",
        "link": "https://cloud.google.com/alloydb/ai",
        "days_ago": 70
    },
    {
        "title": "Dataplex : Gouvernance automatique des lacs de donn√©es",
        "summary": "Nouvelles fonctionnalit√©s d'auto-d√©couverte et de classification des donn√©es sensibles dans Dataplex pour renforcer la s√©curit√©.",
        "provider": "GCP",
        "service": "Dataplex",
        "category": "S√©curit√©",
        "impact_level": 2,
        "impact_analysis": "Indispensable pour la conformit√© PII/GDPR √† l'√©chelle du Petabyte.",
        "link": "https://cloud.google.com/dataplex",
        "days_ago": 115
    },
    {
        "title": "Cloud Storage FUSE : Performances am√©lior√©es pour le training ML",
        "summary": "Mise √† jour du syst√®me de fichiers FUSE pour GCS, optimisant le chargement de donn√©es pour l'entra√Ænement de mod√®les sur GKE et Vertex AI.",
        "provider": "GCP",
        "service": "Cloud Storage",
        "category": "Stockage",
        "impact_level": 1,
        "impact_analysis": "R√©duit les goulots d'√©tranglement I/O pour les gros mod√®les de Deep Learning.",
        "link": "https://cloud.google.com/blog/products/storage/cloud-storage-fuse-csi-driver",
        "days_ago": 145
    },
    {
        "title": "BigQuery Data Clean Rooms : Partage s√©curis√© sans copie",
        "summary": "Disponibilit√© g√©n√©rale des Data Clean Rooms pour partager des donn√©es avec des partenaires externes tout en pr√©servant la confidentialit√©.",
        "provider": "GCP",
        "service": "BigQuery",
        "category": "S√©curit√©",
        "impact_level": 3,
        "impact_analysis": "Facilite les collaborations B2B et l'enrichissement de donn√©es marketing.",
        "link": "https://cloud.google.com/bigquery/docs/data-clean-rooms",
        "days_ago": 80
    }
]

count = 0

for news in historical_news:
    # G√©n√©ration d'une date pass√©e pr√©cise
    past_date = datetime.now() - timedelta(days=news["days_ago"])
    formatted_date = past_date.isoformat()

    # Pr√©paration de l'objet pour Supabase
    data_to_insert = {
        "title": news["title"],
        "link": news["link"], # Lien unique
        "summary": news["summary"],
        "provider": news["provider"],
        "service": news["service"],
        "category": news["category"],
        "impact_level": news["impact_level"],
        "impact_analysis": news["impact_analysis"],
        "raw_source": f"Official {news['provider']} Source",
        "created_at": formatted_date # Date simul√©e r√©aliste
    }

    try:
        # On utilise upsert pour ne pas planter si on relance le script
        # On ignore les doublons potentiels bas√©s sur le lien
        supabase.table("news").upsert(data_to_insert, on_conflict="link").execute()
        print(f"‚úÖ Ajout√© (Il y a {news['days_ago']} jours) : {news['title'][:40]}...")
        count += 1
    except Exception as e:
        print(f"‚ùå Erreur sur {news['title'][:20]}: {e}")

print(f"\nüéâ Termin√© ! {count} articles r√©alistes (5 derniers mois) ont √©t√© ajout√©s.")